model_name: w2v_skip_gram

type_model: skip_gram

optimizer: adam
loss_fn: sparse_categorical_crossentropy
learning_rate: 0.025
lr_scheduler: linear_decrease
epochs: 5
pente: - 0.005
train_steps: 
val_steps: 

train_path: /coding_linux20/programming/datasets/wikitext-2-raw/wiki.train.raw
val_path: /coding_linux20/programming/datasets/wikitext-2-raw/wiki.valid.raw
log_path:  ./word2vec/weights/w2v_skip_gram_1/logs # path from word2vec_tf, because we launch with 'python -m word2vec.train'
weights_path: ./word2vec/weights/w2v_skip_gram_1/

checkpoint_frequency: 

device: /device:GPU:0